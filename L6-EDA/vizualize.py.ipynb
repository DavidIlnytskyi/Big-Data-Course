{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e81411b-77f7-4f34-903b-b9be42da1433",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93bc7586-7fb5-4078-bf49-a318ee616714",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, NumericType, IntegerType\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e987456c-41a2-4c1d-b496-c64281bdab7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1f9fcd9-43dd-4679-a429-de89aab3b4ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_missing_stats_df(df):\n",
    "    total_rows = df.count()\n",
    "    \n",
    "    missing_counts = []\n",
    "    \n",
    "    for col_name, col_type in df.dtypes:\n",
    "        # For string columns, check for null OR empty strings\n",
    "        if col_type == 'string':\n",
    "            missing_expr = F.sum(\n",
    "                F.when(F.col(col_name).isNull() | (F.col(col_name) == \"\"), 1).otherwise(0)\n",
    "            )\n",
    "        # For numeric columns, only check for null (not empty strings)\n",
    "        else:\n",
    "            missing_expr = F.sum(F.when(F.col(col_name).isNull(), 1).otherwise(0))\n",
    "        \n",
    "        missing_counts.append(missing_expr.alias(col_name))\n",
    "    \n",
    "    # Collect results\n",
    "    missing = (\n",
    "        df.select(missing_counts)\n",
    "        .toPandas()\n",
    "        .T\n",
    "        .reset_index()\n",
    "        .rename(columns={\"index\": \"column\", 0: \"missing_count\"})\n",
    "    )\n",
    "    \n",
    "    missing[\"missing_percent\"] = (missing[\"missing_count\"] / total_rows) * 100\n",
    "    missing[\"data_type\"] = [dict(df.dtypes)[col] for col in missing[\"column\"]]\n",
    "    \n",
    "    # Sort by missing count descending for better visibility\n",
    "    missing = missing.sort_values(\"missing_count\", ascending=False)\n",
    "    \n",
    "    return missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82cfb89c-7362-460d-bbac-970886dcb0b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_rows_with_missing_values(df, include_missing_flags=True):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame containing only rows that have at least one missing value.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: Input DataFrame\n",
    "    - include_missing_flags: If True, adds boolean columns showing which fields are missing\n",
    "    \"\"\"\n",
    "    # Create conditions and flag columns\n",
    "    missing_conditions = []\n",
    "    flag_columns = []\n",
    "    \n",
    "    for col_name, col_type in df.dtypes:\n",
    "        if col_type == 'string':\n",
    "            condition = F.col(col_name).isNull() | (F.col(col_name) == \"\")\n",
    "        else:\n",
    "            condition = F.col(col_name).isNull()\n",
    "        \n",
    "        missing_conditions.append(condition)\n",
    "        flag_columns.append(F.when(condition, True).otherwise(False).alias(f\"missing_{col_name}\"))\n",
    "    \n",
    "    # Combined condition for filtering\n",
    "    combined_condition = missing_conditions[0]\n",
    "    for condition in missing_conditions[1:]:\n",
    "        combined_condition = combined_condition | condition\n",
    "    \n",
    "    # Filter rows with missing values\n",
    "    missing_rows_df = df.filter(combined_condition)\n",
    "    \n",
    "    # Add missing flags if requested\n",
    "    if include_missing_flags:\n",
    "        missing_rows_df = missing_rows_df.select(\n",
    "            \"*\", *flag_columns\n",
    "        )\n",
    "    \n",
    "    return missing_rows_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d598238-1f0f-4917-a194-2e3368c64464",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4a2d102-a119-4622-b49c-9b5e78cd7ee6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "columns_to_keep = [\n",
    "    'host_id','host_since','host_is_superhost','latitude','longitude','property_type','room_type','accommodates','bathrooms','bathrooms_text','bedrooms','beds','amenities','price','minimum_nights','maximum_nights', 'number_of_reviews','review_scores_rating','license','instant_bookable','reviews_per_month'\n",
    "]\n",
    "\n",
    "df = spark.read.table('airbnb.raw.listings').select(columns_to_keep)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "337ca70d-f036-47b0-a391-c9c9f37c8070",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a1b4379-f8c5-4379-ac0b-f3106cb91b07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35908c8c-400a-4683-a1d9-aa2e9baa36d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0610a83-63ae-4072-84ef-23ccd447f8d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_missing_stats = get_missing_stats_df(df)\n",
    "display(df_missing_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53fb8938-b5da-4757-b39c-bfd922ab2c23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "As _Bathrooms_ column consists of only null rows, we will remove that to get clearer data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ee4f6b0-53be-495e-aebb-d04da86c30d8",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761078502878}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'host_id','host_since','host_is_superhost','latitude','longitude','property_type','room_type','accommodates', 'bathrooms_text','bedrooms','beds','amenities','price','minimum_nights','maximum_nights', 'number_of_reviews','review_scores_rating','license','instant_bookable','reviews_per_month'\n",
    "]\n",
    "\n",
    "df = df.select(columns)\n",
    "df_missing_rows = get_rows_with_missing_values(df, include_missing_flags=True)\n",
    "print(f\"Rows with missing values: {df_missing_rows.count()}\")\n",
    "print(f\"Total rows in dataset: {df_missing_rows.count()}\")\n",
    "print(f\"Missing rows percentage: {(df_missing_rows.count() / df_missing_rows.count()) * 100:.2f}%\")\n",
    "\n",
    "# Show sample of missing rows\n",
    "display(df_missing_rows.limit(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc140a82-3f4f-4492-a36c-2df44700aed3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff43b5db-e05b-4312-9512-7c8c98b0c58a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Correlation between colums missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d230dfb2-2fe0-4cb5-a61a-e36b791ed5f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "null_indicator_df = df.select([\n",
    "    F.when(F.col(col).isNull(), 0).otherwise(1).alias(col) \n",
    "    for col in df.columns\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fa07e56-2c2b-4e2e-8890-1084111d2514",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Convert to vector column for ML correlation\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=null_indicator_df.columns, \n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "vector_df = assembler.transform(null_indicator_df)\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = Correlation.corr(vector_df, \"features\").head()[0]\n",
    "\n",
    "# Convert to pandas for better visualization\n",
    "corr_array = corr_matrix.toArray()\n",
    "corr_pd = pd.DataFrame(corr_array, \n",
    "                      columns=null_indicator_df.columns, \n",
    "                      index=null_indicator_df.columns)\n",
    "\n",
    "print(corr_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2aa17e4c-e8c6-43b7-8d9b-c19611534cf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a filtered correlation matrix (only strong correlations)\n",
    "threshold = 0\n",
    "filtered_corr = corr_pd.where(abs(corr_pd) > threshold)\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(filtered_corr, \n",
    "            annot=True, \n",
    "            cmap='RdBu_r', \n",
    "            center=0,\n",
    "            fmt='.2f',\n",
    "            square=True)\n",
    "plt.title(f'Missing Data Correlation (|r| > {threshold})')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b264b315-a19a-4d40-960d-dafc09ede159",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Price column analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de4430ab-5a52-4f94-aeb5-33067b5ee461",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Your existing code to get symbol counts\n",
    "symbol_counts = (\n",
    "    df\n",
    "    .select(\n",
    "        F.explode(\n",
    "            F.split(\n",
    "                F.regexp_replace(F.col(\"price\").cast(\"string\"), \"[a-zA-Z0-9\\\\s]\", \"\"),\n",
    "                \"\"\n",
    "            )\n",
    "        ).alias(\"symbol\")\n",
    "    )\n",
    "    .filter(F.col(\"symbol\") != \"\")\n",
    "    .groupBy(\"symbol\")\n",
    "    .count()\n",
    "    .orderBy(F.desc(\"count\"))\n",
    ")\n",
    "\n",
    "# Convert to Pandas for visualization\n",
    "symbol_counts_pd = symbol_counts.toPandas()\n",
    "\n",
    "# Create bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(symbol_counts_pd['symbol'], symbol_counts_pd['count'])\n",
    "plt.title('Symbol Frequency in Price Column')\n",
    "plt.xlabel('Symbols')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print unique symbols\n",
    "symbols_set = {row.symbol for row in symbol_counts.select(\"symbol\").collect()}\n",
    "print(\"Unique symbols in price column:\", symbols_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fb289e3-8228-490b-9fe6-65dca9d04775",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "As we can see, there is no other currencies except dollars. Dataset consists 44 rows which contain \",\" instead of \".\", which can be easily replaced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22782907-0c88-4a70-9e50-a261464e9dd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "vizualize.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
