{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb6d5b50-24b3-404f-9a00-e5b04e96f427",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG samples;\n",
    "USE SCHEMA tpch;\n",
    "\n",
    "USE CATALOG catalog_cp;\n",
    "CREATE SCHEMA IF NOT EXISTS bronze;\n",
    "\n",
    "CREATE OR REPLACE TABLE catalog_cp.bronze.customer AS SELECT * FROM samples.tpch.customer;\n",
    "CREATE OR REPLACE TABLE catalog_cp.bronze.orders AS SELECT * FROM samples.tpch.orders;\n",
    "CREATE OR REPLACE TABLE catalog_cp.bronze.lineitem AS SELECT * FROM samples.tpch.lineitem;\n",
    "CREATE OR REPLACE TABLE catalog_cp.bronze.nation AS SELECT * FROM samples.tpch.nation;\n",
    "CREATE OR REPLACE TABLE catalog_cp.bronze.part AS SELECT * FROM samples.tpch.part;\n",
    "CREATE OR REPLACE TABLE catalog_cp.bronze.partsupp AS SELECT * FROM samples.tpch.partsupp;\n",
    "CREATE OR REPLACE TABLE catalog_cp.bronze.supplier AS SELECT * FROM samples.tpch.supplier;\n",
    "CREATE OR REPLACE TABLE catalog_cp.bronze.region AS SELECT * FROM samples.tpch.region;\n",
    "SELECT COUNT(*) FROM catalog_cp.bronze.customer;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9482fcb-9495-49e9-9da2-bf3fb4336f59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import md5, concat_ws, col, collect_list, lit, when\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "tables = ['customer', 'orders', 'lineitem', 'nation', 'part', 'partsupp', 'supplier', 'region']\n",
    "\n",
    "source_catalog = \"samples\"\n",
    "source_schema = \"tpch\"\n",
    "target_catalog = \"catalog_cp\"\n",
    "target_schema = \"bronze\"\n",
    "\n",
    "def get_table_stats(catalog, schema, table):\n",
    "    df = spark.table(f\"{catalog}.{schema}.{table}\")\n",
    "    row_hashes = df.select(md5(concat_ws(\"||\", *df.columns)).alias(\"row_hash\"))\n",
    "    checksum_df = row_hashes.agg(\n",
    "        md5(concat_ws(\"||\", collect_list(\"row_hash\"))).alias(\"checksum\")\n",
    "    ).withColumn(\"row_count\", lit(df.count())) \\\n",
    "     .withColumn(\"table_name\", lit(table))\n",
    "    return checksum_df.select(\"table_name\", \"row_count\", \"checksum\")\n",
    "\n",
    "validation_dfs = {}\n",
    "\n",
    "all_match = True\n",
    "\n",
    "for table in tables:\n",
    "    source_stats = get_table_stats(source_catalog, source_schema, table) \\\n",
    "        .withColumnRenamed(\"row_count\", \"source_count\") \\\n",
    "        .withColumnRenamed(\"checksum\", \"source_checksum\")\n",
    "    target_stats = get_table_stats(target_catalog, target_schema, table) \\\n",
    "        .withColumnRenamed(\"row_count\", \"target_count\") \\\n",
    "        .withColumnRenamed(\"checksum\", \"target_checksum\")\n",
    "\n",
    "    validation_df = source_stats.join(target_stats, on=\"table_name\", how=\"outer\")\n",
    "\n",
    "    validation_df = validation_df.withColumn(\n",
    "        \"validation_result\",\n",
    "        when((col(\"source_count\") == col(\"target_count\")) & (col(\"source_checksum\") == col(\"target_checksum\")), \"MATCH\")\n",
    "        .when((col(\"source_count\").isNull()) | (col(\"target_count\").isNull()), \"MISSING_TABLE\")\n",
    "        .otherwise(\"MISMATCH\")\n",
    "    )\n",
    "    \n",
    "    validation_dfs[table] = validation_df\n",
    "    \n",
    "    result = validation_df.select(\"validation_result\").collect()[0][0]\n",
    "    print(f\"Table '{table}': {result}\")\n",
    "    \n",
    "    if result != \"MATCH\":\n",
    "        all_match = False\n",
    "\n",
    "if all_match:\n",
    "    print(\"\\nAll tables match.\")\n",
    "else:\n",
    "    print(\"\\nOne or more tables have mismatches or are missing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e14dd3dd-4f7b-4dec-a387-33a783e5fede",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8092482964556366,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "copy_tpch_to_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
